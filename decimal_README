
I need to use long long int to get 64-bit integers, but this is strictly
a C++11 feature - even though it was in the C99 standard and is supported by
most compilers pre-C++11 anyway. I could use a compiler flag or macro
of some kind that I can use to use long int where long long int can't be used,
for maximum portability. (But 32-bit integers are so crappy!)

- - -

NOTES ON EFFICIENCY AND COMPARISON WITH DECIMAL WITH STATIC PLACES

Reading from the underlying integer, and then the number of places, from a
text file (where these are stored as simple integral types written to the
file), is very fast, compared to reading from a string. It is still very fast
even when the number of places varies from one instance to the next, and they
have to be added.

This was only about 25% slower than my fixed point decimal class, to read
1,000,000 numbers, each with a different number of decimal places,
and sum them. It took 0.48 seconds, versus about 0.39 seconds with the
static version.

But the actual adding of the numbers only took roughly 0.05 seconds. The reading
of the numbers from the filestream took most of the time.

The dynamic version took up 16 bytes per number, versus 8 bytes for the
static version.

The advantage of the dynamic version is it is more flexible. You might
occasionally want to determine at runtime that you need to store a particular
number to, say, 8 decimal places. But you don't want to store all numbers
like that, particularly as, during multiplication operations, you run the
risk of overflow.

On the static version, the actual adding of the numbers only took 0.007
seconds. So on the static version, the adding of the numbers was an order of
magnitude faster.

On the dynamic version, the adding of the numbers took 0.05 seconds when they
varied in their number of decimal places. The adding took only 0.014 seconds
when all the numbers had the same number of decimal places. It took 0.021
seconds when 1 in 7 numbers had a different number of decimal places.
This seemed to become 0.010 seconds when I optimised the co_normalize function
a bit for the "all the same" case. I tried inlining the co_normalize function
but it didn't seem to make any difference. I tried inlining the decimal_div
function, but that made it _much slower_! I tried using std::pow in
decimal_div. This required casts, and resulted in the time for adding blowing
out to 0.04 seconds.

The important message, I think, is that when all the numbers have the same
number of decimal places - and it is
likely in an accounting application that the vast majority of numbers will
have the same number of places - then the dynamic version is less than twice
as slow as the static version. But it buys you the flexibility to go to more
decimal places on the odd occasion that that is required.

The other important message is that you can add a million numbers in roughly
a hundredth of a second either way.

  



